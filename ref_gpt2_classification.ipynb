{"cells":[{"cell_type":"markdown","metadata":{"id":"A1bIc0ZAumSJ"},"source":["# Google Drive"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4738,"status":"ok","timestamp":1665877715156,"user":{"displayName":"Matheus Vanzan","userId":"16733324993090982240"},"user_tz":180},"id":"eckl1rAzruSX","outputId":"ef1a3832-f8bc-4501-a323-a07f30b4b302"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["####################################\n","#\n","#  ADD THIS TO EVERY COLAB FILE!\n","#\n","####################################\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import drive.Shareddrives.GPTJ.project.settings as settings\n","\n","PATH_PROJECT = settings.PATH_PROJECT\n","PATH_DATA = settings.PATH_DATA"]},{"cell_type":"code","source":["! cd $PATH_PROJECT && pip install -q -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L5zLhAxca2u4","outputId":"e5205243-1412-4add-95ba-56f87fd131eb","executionInfo":{"status":"ok","timestamp":1665877756879,"user_tz":180,"elapsed":41728,"user":{"displayName":"Matheus Vanzan","userId":"16733324993090982240"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"M5QqmMI7iYnu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665877756880,"user_tz":180,"elapsed":25,"user":{"displayName":"Matheus Vanzan","userId":"16733324993090982240"}},"outputId":"594bb68e-376d-4555-99e3-e2f9c6cd5d9d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Oct 15 23:49:15 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   61C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi\n","\n","# Standard -> Tesla T4\n","# Premium -> Tesla P100-PCIE-16GB"]},{"cell_type":"code","source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","\n","import pandas as pd\n","import torch\n","torch.manual_seed(42)\n","\n","from torch.utils.data import Dataset\n"],"metadata":{"id":"5cR4gcWosrr9","executionInfo":{"status":"ok","timestamp":1665877757432,"user_tz":180,"elapsed":565,"user":{"displayName":"Matheus Vanzan","userId":"16733324993090982240"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Dataset"],"metadata":{"id":"Ijh89fwmrkVK"}},{"cell_type":"code","source":["\n","class SimpleDataset(Dataset):\n","\n","    def __init__(self, contents, labels, tokenizer, max_length=1024):\n","        self.input_ids = []\n","        self.attn_masks = []\n","        self.labels = []\n","\n","        for content, label in zip(contents, labels):\n","\n","            encodings_dict = tokenizer(\n","                content,\n","                truncation=True,\n","                max_length=max_length, \n","                padding='max_length'\n","            )\n","\n","            input_ids = torch.tensor(encodings_dict['input_ids'])    \n","            self.input_ids.append(input_ids)\n","\n","            mask = torch.tensor(encodings_dict['attention_mask'])\n","            self.attn_masks.append(mask)\n","\n","            label = torch.tensor(label)\n","            self.labels.append(label)\n","\n","            # print('item', len(input_ids), '-', input_ids, mask, label)\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return self.input_ids[idx], self.attn_masks[idx], self.labels[idx]\n"],"metadata":{"id":"DXBUeyjSrmCP","executionInfo":{"status":"ok","timestamp":1665877757433,"user_tz":180,"elapsed":7,"user":{"displayName":"Matheus Vanzan","userId":"16733324993090982240"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bVjnEhgiB83l"},"source":["# Treino"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BuoDfJOxLeBP","outputId":"5e3a2ae4-2790-4c11-c14a-b190e55a84a2","executionInfo":{"status":"ok","timestamp":1665877768426,"user_tz":180,"elapsed":10999,"user":{"displayName":"Matheus Vanzan","userId":"16733324993090982240"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPT2LMHeadModel, GPT2ForSequenceClassification\n","\n","model_name = 'gpt2'\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(\n","    model_name,\n","    num_labels = 2 # 0, 1\n",")\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","model = GPT2ForSequenceClassification.from_pretrained(model_name).cuda()\n","model.config.pad_token_id = model.config.eos_token_id\n","\n","# model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","source":["# cache\n","! mkdir -p /content/tmp/\n","! cp -r /content/drive/Shareddrives/GPTJ/data/kaggle/proc-1/* /content/tmp/"],"metadata":{"id":"JNZtFOdUC2JR","executionInfo":{"status":"ok","timestamp":1665878013153,"user_tz":180,"elapsed":244737,"user":{"displayName":"Matheus Vanzan","userId":"16733324993090982240"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(42)\n","\n","from torch.utils.data import random_split\n","\n","labels = [1, 5, 7]\n","path_mask = '/content/tmp/{}/'\n","paths = [path_mask.format(i) for i in labels]\n","\n","def load_dir(path):\n","    contents = []\n","    listdir = list(os.listdir(path)) # [:len_files]\n","    for filename in listdir:\n","        filepath = os.path.join(path, filename)\n","        with open(filepath, 'r') as f:\n","            contents.append(f.read())\n","    print(len(contents), 'files')\n","    return contents\n","\n","train_dataset = None\n","val_dataset = None\n","test_dataset = None\n","\n","for i, label in enumerate(labels):\n","    print('load class', label, 'as', i)\n","    contents = load_dir(paths[i])\n","    labels = [i]*len(contents)\n","    dataset_i = SimpleDataset(contents, labels, tokenizer)\n","    print('len', len(dataset_i))\n","\n","    size_10 = int(0.1 * len(dataset_i))\n","    size_80 = len(dataset_i) - 2*size_10\n","    train_dataset_i, val_dataset_i, test_dataset_i = \\\n","        random_split(dataset_i, [size_10, size_10, size_80])\n","\n","    if not train_dataset:\n","        train_dataset = train_dataset_i\n","        val_dataset = val_dataset_i\n","        test_dataset = test_dataset_i\n","    else:\n","        train_dataset += train_dataset_i\n","        val_dataset += val_dataset_i\n","        test_dataset += test_dataset_i\n","\n","# print('total len', len(dataset))\n","\n","# size_10 = int(0.1 * len(dataset))\n","# size_80 = len(dataset) - 2*size_10\n","\n","# train_dataset, val_dataset, test_dataset = random_split(dataset, [size_10, size_10, size_80])\n","\n","print(train_dataset[-1])\n","print(val_dataset[-1])\n","print(test_dataset[-1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iL1OqfiTuVLY","outputId":"c5dc3c90-d8cc-4ba4-d85d-aad0bd799bea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["load class 1 as 0\n","1541 files\n"]}]},{"cell_type":"code","source":["# (tensor([  562,  2454, 50115,  ...,  9940,    87,  3571]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor(1))\n","# (tensor([  562,  2454, 50115,  ...,  1409,  1225,    72]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor(1))\n","# (tensor([  562,  2454, 50115,  ...,    72,  3571, 21844]), tensor([1, 1, 1,  ..., 1, 1, 1]), tensor(1))"],"metadata":{"id":"1GlV2wKBme4_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir='/content/',\n","    num_train_epochs=2,\n","    # logging_steps=5000,\n","    # save_steps=5000,                                   \n","    per_device_train_batch_size=2,\n","    per_device_eval_batch_size=2,\n","    warmup_steps=0,\n","    weight_decay=0.01,  \n","    # logging_dir=os.path.join(PATH_DATA, model_name, 'logs')\n",")\n","\n","trainer = Trainer(\n","    model=model, \n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset, \n","    # This custom collate function is necessary \n","    # to built batches of data\n","    data_collator=lambda data: {\n","        'input_ids': torch.stack([f[0] for f in data]),       \n","        'attention_mask': torch.stack([f[1] for f in data]),\n","        'labels': torch.stack([f[2] for f in data])\n","    }\n",")\n","# Start training process!\n","trainer.train()"],"metadata":{"id":"wlSzjzkbtXmD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Classification"],"metadata":{"id":"xp0tlCKNR2e9"}},{"cell_type":"code","source":["import numpy as np\n","from datasets import load_metric\n","metric = load_metric(\"accuracy\")\n","\n","test_predictions_weights = trainer.predict(test_dataset)\n","test_predictions = np.argmax(test_predictions_weights[0], axis=1)\n","# print(test_predictions)\n","\n","test_labels = [label for _, _, label in test_dataset]\n","# print(test_labels)\n","\n","test_references = np.array(test_labels)\n","\n","metric.compute(predictions=test_predictions, references=test_references)\n","# {'accuracy': 0.91888}"],"metadata":{"id":"A_zB6hBNR0An"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["A1bIc0ZAumSJ","JuIxLRfousPd","ZZdJjTltuR4z"],"provenance":[{"file_id":"1z5YmbSqayjX1AVwNGdk4fiusOkBxa_YK","timestamp":1663640920362}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}